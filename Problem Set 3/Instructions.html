<!DOCTYPE html>
<html>
<head>
<title>Instructions.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="problem-set-3-reinforcement-learning">Problem Set 3: Reinforcement Learning</h1>
<p>The goal of this problem set is to implement and get familiar with Reinforcement Learning.</p>
<p>To run the autograder, type the following command in the terminal:</p>
<pre><code>python autograder.py
</code></pre>
<p>If you wish to run a certain problem only (e.g. problem 1), type:</p>
<pre><code>python autograder.py -q 1
</code></pre>
<p>where 1 is the number of the problem you wish to run.</p>
<p>You can also specify a single testcase only (e.g. testcase01.json in problem 1) by typing:</p>
<pre><code>python autograder.py -q 1/testcase01.json
</code></pre>
<p>To debug your code through the autograder, you should disable the timeout functionality. This can be done via the <code>debug</code> flag as follow:</p>
<pre><code>python autograder.py -d -q 1/testcase01.json
</code></pre>
<p>Or you could set a time scale to increase or decrease your time limit. For example, to half your time limits, type:</p>
<pre><code>python autograder.py -t 0.5 -q 1/testcase01.json
</code></pre>
<p><strong>Note:</strong> Your machine may be faster or slower than the grading device. To automatically detect your machine's speed, the autograder will run <code>speed_test.py</code> to measure your machine's relative speed, then it will scale the time limits automatically. The speed test result is automatically stored in <code>time_config.json</code> to avoid running the speed test every time you run the autograder. If you want to re-calculate your machine's speed, you can do so by either running <code>speed_test.py</code>, or deleting <code>time_config.json</code> followed by running the autograder.</p>
<h2 id="instructions">Instructions</h2>
<p>In the attached python files, you will find locations marked with:</p>
<pre><code>#TODO: ADD YOUR CODE HERE
utils.NotImplemented()
</code></pre>
<p>Remove the <code>utils.NotImplemented()</code> call and write your solution to the problem. <strong>DO NOT MODIFY ANY OTHER CODE</strong>; The grading of the assignment will be automated and any code written outside the assigned locations will not be included during the grading process.</p>
<p><strong>IMPORTANT</strong>: In this problem set, you must document your code (explain the algorithm you are implementing in your own words within the code) to get the full grade. Undocumented code will be penalized. Imagine that you are in a discussion, and that your documentation are answers to all the questions that you could be asked about your implementation (e.g. why choose something as a data structure, what purpose does conditions on <code>if</code>, <code>while</code> and <code>for</code> blocks serve, etc.).</p>
<p><strong>IMPORTANT</strong>: For this assignment, you can only use the <strong>Built-in Python Modules</strong>. Do not use external libraries such as <code>Numpy</code>, <code>Scipy</code>, etc. You can check if a module is builtin or not by looking up <a href="https://docs.python.org/3/library/">The Python Standard Library</a> page.</p>
<h2 id="file-structure">File Structure</h2>
<p>There are 5 files that you can run from the terminal:</p>
<ul>
<li><code>play_grid.py</code>: where you can play a grid or let an agent play it. This is useful for debugging.</li>
<li><code>train_grid.py</code>: where you can train a solver to play a grid. This is useful for debugging.</li>
<li><code>play_snake.py</code>: where you can play a snake game or let a random agent play it. This is useful for debugging.</li>
<li><code>autograder.py</code>: where you test your code and get feedback about your results for the test cases.</li>
<li><code>speed_test.py</code>: where you can check your computer's speed, which is useful to predict how long your code will take to run on the grading machine.</li>
</ul>
<p>These are the files relevant to the requirements:</p>
<ul>
<li><code>mathutils.py</code> <strong>[IMPORTANT]</strong>: This contains some useful math utilities. You should understand how to use the classes and functions written in it.</li>
<li><code>mdp.py</code> <strong>[IMPORTANT]</strong>: This is where the generic MDP (Markov Decision Process) is defined. You should understand the code written in it.</li>
<li><code>environment.py</code> <strong>[IMPORTANT]</strong>: This is where the generic environment is defined. It wraps an MDP and simulates episodes. This is used to train and run RL agents. You should understand the code written in it.</li>
<li><code>grid.py</code>: This is where the grid MDP and environment are defined.</li>
<li><code>snake.py</code> <strong>[IMPORTANT + REQUIREMENT]</strong>: This is where the snake environment is defined. You should complete the code in this file.</li>
<li><code>base_rl.py</code> <strong>[IMPORTANT]</strong>: This defines some base classes and definitions needed by other RL code, including the base class for the feature extractor. You should understand the code written in it.</li>
<li><code>features_grid.py</code>: This is where the features, needed to create the Approximate Q-Learning agent for the grid environment, are defined.</li>
<li><code>agents.py</code> <strong>[IMPORTANT]</strong>: This is where game agents are defined. You should understand the code written in it.</li>
<li><code>value_iteration.py</code> <strong>[IMPORTANT + REQUIREMENT]</strong>: This is where the value iteration agent is defined. You should complete the code in this file.</li>
<li><code>options.py</code> <strong>[IMPORTANT + REQUIREMENT]</strong>: This is where the functions for problem 2 are defined. You should complete the code in this file. You must document your code well, specially in this part.</li>
<li><code>reinforcement_learning.py</code> <strong>[IMPORTANT + REQUIREMENT]</strong>: This is where the RL agents (SARSA, Q &amp; Approximate Q) are defined. You should complete the code in this file.</li>
<li><code>training_loops.py</code> <strong>[IMPORTANT]</strong>: This is where the training loops for the RL agents are defined. It is recommended that you understand it.</li>
</ul>
<p>There are some files defined in the folder <code>helpers</code> that are used for testing. You should not need to understand how to use them, but it won't harm to know the following information about them:</p>
<ul>
<li><code>globals.py</code>: This only contains some imports that should be seen by all the testing code, so they are defined here to be imported in <code>autograder.py</code>.</li>
<li><code>mt19937.py</code>: This is a pseudo-random number generator. We define our own instead of using the builtin <code>random</code> module to ensure that the results are reproduceable regardless of the Python version.</li>
<li><code>rl_utils.py</code>: This contains some utility functions to compare and print the results onto the console.</li>
<li><code>test_tools.py</code>: This is where most of the testing code lies. It has pairs of functions <code>run_*</code> and <code>compare_*</code> to run and check the results of different functions in the requirements. It is relatively complex, and error messages may point you towards this file, if your code returns something wrong that also leads the testing code to crash.</li>
<li><code>utils.py</code>: This contains some classes and functions that are used by <code>autograder.py</code> and <code>test_tools.py</code>. This is where the <code>load_function</code> lies which is used to dynamically load your solutions from their python files at runtime without using <code>import</code>. This ensures that having an error in one file does not stop the autograder from grading the other files.</li>
</ul>
<hr>
<h2 id="problem-definitions">Problem Definitions</h2>
<p>There are two environments in this problem set:</p>
<ol>
<li><strong>Grid World</strong>: where the environment is a 2D grid, and the player can move in one of 4 directions (<code>U</code>, <code>D</code>, <code>L</code>, <code>R</code>). The actions are noisy, so the end result may be moving along one of the 2 directions orthogonal to the desired direction. Some locations <code>&quot;#&quot;</code> are occupied with walls, so the player cannot stand on them. Some other locations <code>&quot;T&quot;</code> are terminal states which ends the episode as soon as the player stands on them. Each location has an associated reward which is given to the player if they do an action that gets them to be in that state in the next time step. The Markov decision process and environment of the grid world is implemented in <code>grid.py</code> and the environment instances are included in the <code>grids</code> folder.</li>
</ol>
<p>You can play a grid world game by running:</p>
<pre><code># For playing a grid (e.g. grid1.json)  
python play_grid.py grids\grid1.json
</code></pre>
<p>You can also let a learning agent play the game in your place (e.g. a Q-Learning Agent) as follows:</p>
<pre><code>python play_grid.py grids\grid1.json -a q_learning -m models/model.json
</code></pre>
<p><strong>NOTE:</strong> In addition to the agent, we supply a model file from which the agent will read its data (e.g. Q-values for Q-learing &amp; SARSA agents, weights for approximate Q-learning and Utilities for value iteration agents). If we don't supply a model file, the agent will play using the initial values of their learnable parameters.</p>
<p>To train an agent and save its data in model file, use <code>train.py</code> as follows:</p>
<pre><code># For training a q_learning agent on grid1.json for 1000 iterations where each episode is limited to 100 steps only
python train_grid.py q_learning grids\grid1.json models/model.json -i 1000 -sl 100
</code></pre>
<p>The agent options are:</p>
<ul>
<li><code>human</code> where the human play via the console</li>
<li><code>random</code> where the computer plays randomly</li>
<li><code>value_iteration</code> where the agent uses the learned utilities (via value iteration) and the MDP to decide the action to take.</li>
<li><code>sarsa</code> where the agent uses the learned Q-value (via SARSA) to decide the action to take.</li>
<li><code>q_learning</code> where the agent uses the learned Q-value (via Q-Learning) to decide the action to take.</li>
<li><code>q_learning_approx</code> where the agent uses the learned weights (via Linear Approximate Q-Learning) to decide the action to take.</li>
</ul>
<p>To get detailed help messages, run <code>play_grid.py</code> and <code>train_grid.py</code> with the <code>-h</code> flag.</p>
<ol start="2">
<li><strong>Snake</strong>: where the environment is a 2D grid, and the snake can move in one of 4 directions (<code>U</code>, <code>D</code>, <code>L</code>, <code>R</code>). Initially, the snake has a length of 1 (occupies one cell only), is located at the center of the map and is moving LEFT. If the head enters a cell containing an apple, it eats the apple, its length increase by 1, and it gets a reward of <code>1</code>. If the snake head exits the level area, it wraps around and enters the level from the other side (e.g., if it exits the level from the top, it enters the level from bottom). If the snake bites itself (the head enters a cell occupied by its own body), it loses and gets a reward of <code>-100</code>. The possible actions are <code>NONE</code>, where the snake keeps moving in its current direction, and the two directions orthogonal to its current direction, in which case the snake changes its direction and starts moving in that new direction starting from the current time step. For the snake game, we do not define a Markov decision process, since the number of states are huge, and it would be impractical to list all of them. Instead, we define an environment only which is defined in <code>snake.py</code>, but it is incomplete, and you need to finish some <code>TODO</code>s to get it working.</li>
</ol>
<p>You can try to play snake here: <a href="https://www.googlesnake.com/">https://www.googlesnake.com/</a>
The only difference between google snake and our implementation is that the snake wraps around the level instead of dying when it hits a wall.</p>
<p>After you finish, the snake environment requirement, you can play a snake game by running:</p>
<pre><code># For playing a 5x5 snake game  
python play_snake.py 5 5
</code></pre>
<p>You can also let a random agent play the game in your place as follows:</p>
<pre><code>python play_snake.py 5 5 -a random
</code></pre>
<p><strong>NOTE:</strong> Currently, there is no training script or feature extractor for the snake game, so RL agents are not supported.</p>
<p>To get detailed help messages, run <code>play_snake.py</code> with the <code>-h</code> flag.</p>
<hr>
<h2 id="important-notes">Important Notes</h2>
<p>This problem set relies a lot on randomness and Reinforcement Learning usually does not converge to the same results when different random seeds are used. So it is essential to follow the instructions written in the comments around the TODOs to acheive the same results. In addition, while acting, if two actions have the same value (Q-value, expected utilities, etc.), pick the action that appears first in the list returned by <code>mdp.get_actions</code> or <code>env.actions()</code>.</p>
<hr>
<h2 id="problem-1-value-iteration">Problem 1: Value Iteration</h2>
<p>Inside <code>value_iteration.py</code>, modify the functions marked by a <code>**TODO**</code> to complete the <code>ValueIterationAgent</code> class. Note that the Reward <code>R(s, a, s')</code> is a function of the current state, the action and the next state, so use the appropriate version of the bellman equation:</p>
<p>$$U(s) = \max_{a} \sum_{s'} P(s'|s,a) [ R(s,a,s') + \gamma U(s')]$$</p>
<h2 id="problem-2-parameter-selection">Problem 2: Parameter Selection</h2>
<p><strong>WARNING</strong>: The problem will run your solution for value iteration problem. So please make sure that your value iteration code is correct before proceeding to this problem.</p>
<p><img src="docs/MDP.png" alt="MDP"></p>
<p>As shown in  figure above, this MDP has 2 terminal states with positive rewards (one is close to the start with a reward of +1 and far from the start with a reward of +10). To reach any of these 2 states, the agent can either take a short yet dangerous path (going directly right) or take a long yet safe path (going up, then right, then down). The shorter path is dangerous since it extends alongside a row of terminal states with a penalty of -10 each.</p>
<p>The goal of this question is to select values for these 3 parameters (action noise, discount factor and living reward) to control the policy.</p>
<ul>
<li>The action noise is the probability that the actual direction of movement ends up being along one of the 2 directions orthogonal to the desired direction. For example, if the noise is 0.2 and the action is up, then the agent has an (1-0.2)=0.8 chance to actually go up and (0.2/2)=0.1 to go left and (0.2/2)=0.1 to go right.</li>
<li>The discount factor is the gamma as described in the bellman equation.</li>
<li>The living reward is the reward for going to a non-terminal state. This reward can be positive or negative.</li>
</ul>
<p>In the file <code>options.py</code>, there are 6 functions <code>question2_1</code> to <code>question2_6</code> where each of them returns a dictionary containing the 3 parameters described above. The goal is to select values for there parameters such that the policy behaves as follows:</p>
<ol>
<li>For <code>question2_1</code>, we want the policy to seek the near terminal state (reward +1) via the short dangerous path (moving besides the row of -10 state).</li>
<li>For <code>question2_2</code>, we want the policy to seek the near terminal state (reward +1) via the long safe path (moving away from the row of -10 state).</li>
<li>For <code>question2_3</code>, we want the policy to seek the far terminal state (reward +10) via the short dangerous path (moving besides the row of -10 state).</li>
<li>For <code>question2_4</code>, we want the policy to seek the far terminal state (reward +10) via the long safe path (moving away from the row of -10 state).</li>
<li>For <code>question2_5</code>, we want the policy to avoid any terminal state and keep the episode going on forever.</li>
<li>For <code>question2_6</code>, we want the policy to seek any terminal state (even ones with the -10 penalty) and try to end the episode in the shortest time possible.</li>
</ol>
<h2 id="problem-3-sarsa">Problem 3: SARSA</h2>
<p>Inside <code>reinforcement_learning.py</code>, modify the functions marked by a <code>**TODO**</code> to complete the <code>SARSALearningAgent</code> class.</p>
<p>$$Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma Q(s',a') - Q(s,a))$$</p>
<h2 id="problem-4-q-learning">Problem 4: Q-Learning</h2>
<p>Inside <code>reinforcement_learning.py</code>, modify the functions marked by a <code>**TODO**</code> to complete the <code>QLearningAgent</code> class.</p>
<p>$$Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma \max_{a'}Q(s',a') - Q(s,a))$$</p>
<h2 id="problem-5-approximate-q-learning">Problem 5: Approximate Q-Learning</h2>
<p>Inside <code>reinforcement_learning.py</code>, modify the functions marked by a <code>**TODO**</code> to complete the <code>ApproximateQLearningAgent</code> class.</p>
<p>$$w_{ia} \leftarrow w_{ia} + \alpha(r + \gamma \max_{a'}Q(s',a') - Q(s,a))w_i$$</p>
<p>where $w_{ia}$ is the the weight of the feature $x_i$ in $Q(s,a)$ and ${x_1, x_2, ..., x_n}$ are the features of the state $s$. Thus the approximate Q-function can be written as follows:</p>
<p>$$Q(s,a) = \sum_i w_{ia}*x_i$$</p>
<h2 id="problem-6-snake-environment">Problem 6: Snake Environment</h2>
<p>Inside <code>snake.py</code>, modify the functions marked by a <code>**TODO**</code> to complete the <code>SnakeEnv</code> class.</p>
<h2 id="delivery">Delivery</h2>
<p><strong>IMPORTANT</strong>: You must fill the <strong><code>student_info.json</code></strong> file since it will be used to identify you as the owner of this work. The most important field is the <strong>id</strong> which will be used by the automatic grader to identify you. You also must compress the solved python files and the <strong><code>student_info.json</code></strong> file together in a <strong>zip</strong> archive so that the autograder can associate your solution files with the correct <strong><code>student_info.json</code></strong> file. The failure to abide with the these requirements will lead to a zero since your submission will not be graded.</p>
<p>For this assignment, you should submit the following files only:</p>
<ul>
<li><code>student_info.json</code></li>
<li><code>value_iteration.py</code></li>
<li><code>options.py</code></li>
<li><code>reinforcement_learning.py</code></li>
<li><code>snake.py</code></li>
</ul>
<p>Put these files in a compressed zip file named <code>solution.zip</code> which you should submit to Google Classroom.</p>
<p>The delivery deadline is <code>Sunday December 15th 2024 23:59</code>. It should be delivered on <strong>Google Classroom</strong>. This is an individual assignment. The delivered code should be solely written by the student who delivered it. Any evidence of plagiarism will lead to receiving <strong>zero</strong> points.</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });</script>
</body>
</html>
